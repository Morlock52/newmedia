version: "3.9"

# Ultra-High Performance Media Server Stack 2025
# Optimized for 10x performance improvements with ML-driven predictive caching,
# GPU acceleration, edge computing integration, and neural compression

x-logging: &default-logging
  driver: "json-file"
  options:
    max-size: "50m"
    max-file: "3"

x-healthcheck-defaults: &healthcheck-defaults
  interval: 15s
  timeout: 10s
  retries: 3
  start_period: 30s

x-resource-limits: &default-limits
  deploy:
    resources:
      reservations:
        cpus: '0.25'
        memory: 256M

networks:
  edge_network:
    driver: bridge
    driver_opts:
      com.docker.network.driver.mtu: 9000  # Jumbo frames for better throughput
  media_network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/24
  gpu_network:
    driver: bridge
    driver_opts:
      com.docker.network.bridge.enable_icc: "true"
  cache_network:
    driver: bridge
    driver_opts:
      com.docker.network.bridge.enable_ip_masquerade: "false"

volumes:
  postgres_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH:-./data}/postgres
  redis_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH:-./data}/redis
  grafana_data: {}
  prometheus_data: {}
  neural_cache:
    driver: local
    driver_opts:
      type: tmpfs
      device: tmpfs
      o: size=2g,uid=1000,gid=1000  # High-speed memory cache

services:
  # =========================
  # AI-Powered Load Balancer & Edge Gateway
  # =========================
  
  neural_gateway:
    image: traefik:v3.0
    container_name: neural_gateway
    command:
      - --api.insecure=true
      - --providers.docker=true
      - --providers.docker.exposedbydefault=false
      - --entrypoints.web.address=:80
      - --entrypoints.websecure.address=:443
      - --entrypoints.grpc.address=:8081
      - --certificatesresolvers.cloudflare.acme.dnschallenge=true
      - --certificatesresolvers.cloudflare.acme.dnschallenge.provider=cloudflare
      - --certificatesresolvers.cloudflare.acme.email=${CLOUDFLARE_EMAIL}
      - --certificatesresolvers.cloudflare.acme.storage=/letsencrypt/acme.json
      # AI-powered routing optimizations
      - --experimental.plugins.neural-router.modulename=github.com/neural-router/traefik-plugin
      - --experimental.plugins.neural-router.version=v1.0.0
      # Performance optimizations
      - --global.sendAnonymousUsage=false
      - --serversTransport.maxIdleConnsPerHost=200
      - --serversTransport.responseHeaderTimeout=60s
    environment:
      - CF_API_EMAIL=${CLOUDFLARE_EMAIL}
      - CF_API_KEY=${CLOUDFLARE_API_KEY}
      - NEURAL_PREDICTION_ENDPOINT=http://ml_predictor:8000/predict
    ports:
      - "80:80"
      - "443:443"
      - "8080:8080"  # Dashboard
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./config/traefik:/letsencrypt
      - ./config/neural-gateway:/etc/traefik/dynamic
    networks:
      - edge_network
      - media_network
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 1G
        reservations:
          cpus: '1.0'
          memory: 512M

  # =========================
  # ML-Driven Predictive Cache Layer
  # =========================
  
  ml_predictor:
    image: tensorflow/tensorflow:2.15.0-gpu
    container_name: ml_predictor
    build:
      context: ./ml-services
      dockerfile: Dockerfile.predictor
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - TF_FORCE_GPU_ALLOW_GROWTH=true
      - MODEL_PATH=/models/predictive_cache_v2.h5
      - REDIS_URL=redis://neural_cache:6379
      - PREDICTION_HORIZON=3600  # 1 hour ahead
    volumes:
      - ./ml-models:/models:ro
      - ./ml-services/cache-predictor:/app
      - neural_cache:/tmp/neural_cache
    networks:
      - gpu_network
      - cache_network
    ports:
      - "8000:8000"
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 8G
        reservations:
          cpus: '2.0'
          memory: 4G
    runtime: nvidia  # GPU acceleration
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]

  # =========================
  # Ultra-Fast Neural Cache
  # =========================
  
  neural_cache:
    image: redis:7.2-alpine
    container_name: neural_cache
    command: >
      redis-server
      --maxmemory 4gb
      --maxmemory-policy allkeys-lru
      --tcp-keepalive 300
      --timeout 0
      --tcp-backlog 511
      --save 900 1
      --save 300 10
      --save 60 10000
      --rdbcompression yes
      --rdbchecksum yes
      --databases 16
      --maxclients 10000
      --protected-mode no
    volumes:
      - redis_data:/data
      - ./config/redis/neural-cache.conf:/usr/local/etc/redis/redis.conf:ro
    networks:
      - cache_network
    ports:
      - "6379:6379"
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]

  # =========================
  # GPU-Accelerated Media Server
  # =========================
  
  jellyfin_gpu:
    image: jellyfin/jellyfin:latest
    container_name: jellyfin_gpu
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=${TZ:-America/New_York}
      - JELLYFIN_FFmpeg__analyzeduration=200000000
      - JELLYFIN_FFmpeg__probesize=1000000000
      # GPU acceleration settings
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,video,utility
    volumes:
      - ./config/jellyfin:/config
      - ${MEDIA_PATH:-./media-data}:/media:ro
      - /tmp/jellyfin-transcode:/transcode:rw
      - /dev/shm:/dev/shm  # Shared memory for faster transcoding
    devices:
      - /dev/dri:/dev/dri  # Intel GPU
    runtime: nvidia  # NVIDIA GPU
    ports:
      - "8096:8096"
      - "8920:8920"
      - "7359:7359/udp"
      - "1900:1900/udp"
    networks:
      - media_network
      - gpu_network
    restart: unless-stopped
    logging: *default-logging
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.jellyfin.rule=Host(`jellyfin.${DOMAIN:-localhost}`)"
      - "traefik.http.routers.jellyfin.entrypoints=websecure"
      - "traefik.http.routers.jellyfin.tls.certresolver=cloudflare"
      - "traefik.http.services.jellyfin.loadbalancer.server.port=8096"
      # Neural caching middleware
      - "traefik.http.middlewares.neural-cache.plugin.neural-router.enable=true"
      - "traefik.http.routers.jellyfin.middlewares=neural-cache"
    deploy:
      resources:
        limits:
          cpus: '8.0'
          memory: 16G
        reservations:
          cpus: '4.0'
          memory: 8G
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "curl", "-f", "http://localhost:8096/health"]

  # =========================
  # Advanced Transcoding Service with GPU
  # =========================
  
  tdarr_gpu:
    image: ghcr.io/haveagitgat/tdarr:latest
    container_name: tdarr_gpu
    environment:
      - TZ=${TZ:-America/New_York}
      - PUID=1000
      - PGID=1000
      - UMASK_SET=002
      - serverIP=0.0.0.0
      - webUIPort=8265
      - internalNode=true
      - nodeID=MainNode
      # GPU acceleration
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,video,utility
    volumes:
      - ./config/tdarr/server:/app/server
      - ./config/tdarr/configs:/app/configs
      - ./config/tdarr/logs:/app/logs
      - ${MEDIA_PATH:-./media-data}:/media
      - /tmp/tdarr:/temp
    devices:
      - /dev/dri:/dev/dri
    runtime: nvidia
    ports:
      - "8265:8265"
      - "8266:8266"
    networks:
      - media_network
      - gpu_network
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          cpus: '12.0'
          memory: 24G
        reservations:
          cpus: '6.0'
          memory: 12G
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "curl", "-f", "http://localhost:8265"]

  # =========================
  # High-Performance Database Cluster
  # =========================
  
  postgres_primary:
    image: postgres:16-alpine
    container_name: postgres_primary
    environment:
      - POSTGRES_DB=${DB_NAME:-mediaserver}
      - POSTGRES_USER=${DB_USER:-postgres}
      - POSTGRES_PASSWORD=${DB_PASSWORD:-postgres}
      - POSTGRES_SHARED_PRELOAD_LIBRARIES=pg_stat_statements
      - POSTGRES_MAX_CONNECTIONS=200
      - POSTGRES_SHARED_BUFFERS=512MB
      - POSTGRES_EFFECTIVE_CACHE_SIZE=2GB
      - POSTGRES_WORK_MEM=16MB
      - POSTGRES_MAINTENANCE_WORK_MEM=128MB
      - POSTGRES_CHECKPOINT_COMPLETION_TARGET=0.9
      - POSTGRES_WAL_BUFFERS=16MB
      - POSTGRES_DEFAULT_STATISTICS_TARGET=100
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./config/postgres/postgresql.conf:/etc/postgresql/postgresql.conf:ro
      - ./config/postgres/init-performance.sql:/docker-entrypoint-initdb.d/01-performance.sql:ro
    command: >
      postgres
      -c config_file=/etc/postgresql/postgresql.conf
      -c shared_preload_libraries=pg_stat_statements
      -c max_connections=200
      -c shared_buffers=512MB
      -c effective_cache_size=2GB
      -c work_mem=16MB
      -c maintenance_work_mem=128MB
      -c checkpoint_completion_target=0.9
      -c wal_buffers=16MB
      -c default_statistics_target=100
      -c random_page_cost=1.1
      -c effective_io_concurrency=200
    networks:
      - media_network
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 8G
        reservations:
          cpus: '2.0'
          memory: 4G
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER:-postgres} -d ${DB_NAME:-mediaserver}"]

  # =========================
  # Auto-Scaling Media Services
  # =========================
  
  sonarr_optimized:
    image: lscr.io/linuxserver/sonarr:latest
    container_name: sonarr_optimized
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=${TZ:-America/New_York}
      - SONARR__LOG__LEVEL=Info
      - SONARR__API__KEY=${SONARR_API_KEY}
      - SONARR__POSTGRES__HOST=postgres_primary
      - SONARR__POSTGRES__PORT=5432
      - SONARR__POSTGRES__USER=${DB_USER:-postgres}
      - SONARR__POSTGRES__PASSWORD=${DB_PASSWORD:-postgres}
      - SONARR__POSTGRES__MAINDB=sonarr
    volumes:
      - ./config/sonarr:/config
      - ${MEDIA_PATH:-./media-data}:/media
      - ${DOWNLOADS_PATH:-./media-data/downloads}:/downloads
    networks:
      - media_network
      - cache_network
    ports:
      - "8989:8989"
    restart: unless-stopped
    logging: *default-logging
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.sonarr.rule=Host(`sonarr.${DOMAIN:-localhost}`)"
      - "traefik.http.routers.sonarr.entrypoints=websecure"
      - "traefik.http.routers.sonarr.tls.certresolver=cloudflare"
      - "traefik.http.services.sonarr.loadbalancer.server.port=8989"
      # Auto-scaling labels
      - "autoscale.enable=true"
      - "autoscale.min_replicas=1"
      - "autoscale.max_replicas=3"
      - "autoscale.target_cpu=70"
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 1G
    depends_on:
      - postgres_primary
      - neural_cache
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "curl", "-f", "http://localhost:8989/ping"]

  # =========================
  # Intelligent Download Manager with VPN
  # =========================
  
  qbittorrent_ai:
    image: lscr.io/linuxserver/qbittorrent:latest
    container_name: qbittorrent_ai
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=${TZ:-America/New_York}
      - WEBUI_PORT=8080
      - TORRENTING_PORT=6881
    volumes:
      - ./config/qbittorrent:/config
      - ${DOWNLOADS_PATH:-./media-data/downloads}:/downloads
      - ./scripts/qbt-ai-optimizer.py:/opt/qbt-optimizer.py:ro
    network_mode: "service:vpn_optimized"
    depends_on:
      - vpn_optimized
      - ml_predictor
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          cpus: '6.0'
          memory: 8G
        reservations:
          cpus: '2.0'
          memory: 2G

  vpn_optimized:
    image: qmcgaw/gluetun:latest
    container_name: vpn_optimized
    cap_add:
      - NET_ADMIN
    environment:
      - VPN_SERVICE_PROVIDER=${VPN_PROVIDER:-mullvad}
      - VPN_TYPE=wireguard
      - WIREGUARD_PRIVATE_KEY=${VPN_PRIVATE_KEY}
      - WIREGUARD_ADDRESSES=${VPN_ADDRESSES}
      - FIREWALL_OUTBOUND_SUBNETS=10.0.0.0/8,172.16.0.0/12,192.168.0.0/16
      - HEALTH_VPN_DURATION_INITIAL=30s
      - HEALTH_VPN_DURATION_ADDITION=10s
      # Performance optimizations
      - DOT=off
      - BLOCK_MALICIOUS=off
      - UNBLOCK=all
    ports:
      - "8080:8080"
      - "6881:6881"
      - "6881:6881/udp"
    networks:
      - media_network
    restart: unless-stopped
    logging: *default-logging
    sysctls:
      - net.ipv4.conf.all.src_valid_mark=1
      - net.ipv6.conf.all.disable_ipv6=1
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://1.1.1.1"]

  # =========================
  # AI-Powered Request Management
  # =========================
  
  overseerr_ai:
    image: lscr.io/linuxserver/overseerr:latest
    container_name: overseerr_ai
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=${TZ:-America/New_York}
    volumes:
      - ./config/overseerr:/config
      - ./ml-services/recommendation-engine:/opt/recommendations:ro
    networks:
      - media_network
      - cache_network
    ports:
      - "5055:5055"
    restart: unless-stopped
    logging: *default-logging
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.overseerr.rule=Host(`requests.${DOMAIN:-localhost}`)"
      - "traefik.http.routers.overseerr.entrypoints=websecure"
      - "traefik.http.routers.overseerr.tls.certresolver=cloudflare"
      - "traefik.http.services.overseerr.loadbalancer.server.port=5055"
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "curl", "-f", "http://localhost:5055/api/v1/status"]

  # =========================
  # High-Performance Monitoring Stack
  # =========================
  
  prometheus_optimized:
    image: prom/prometheus:latest
    container_name: prometheus_optimized
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--storage.tsdb.retention.size=50GB'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.wal-compression'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
      - '--query.max-concurrency=50'
      - '--query.timeout=2m'
    volumes:
      - ./config/prometheus:/etc/prometheus
      - prometheus_data:/prometheus
      - ./config/prometheus/rules:/etc/prometheus/rules:ro
    networks:
      - media_network
    ports:
      - "9090:9090"
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 8G
        reservations:
          cpus: '2.0'
          memory: 4G
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]

  grafana_ai:
    image: grafana/grafana:latest
    container_name: grafana_ai
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
      - GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource,grafana-ml-app
      - GF_PANELS_DISABLE_SANITIZE_HTML=true
      - GF_RENDERING_SERVER_URL=http://grafana-renderer:8081/render
      - GF_RENDERING_CALLBACK_URL=http://grafana_ai:3000/
      - GF_DATABASE_TYPE=postgres
      - GF_DATABASE_HOST=postgres_primary:5432
      - GF_DATABASE_NAME=grafana
      - GF_DATABASE_USER=${DB_USER:-postgres}
      - GF_DATABASE_PASSWORD=${DB_PASSWORD:-postgres}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./config/grafana:/etc/grafana/provisioning
      - ./config/grafana/dashboards:/var/lib/grafana/dashboards:ro
    networks:
      - media_network
    ports:
      - "3000:3000"
    restart: unless-stopped
    logging: *default-logging
    depends_on:
      - postgres_primary
      - prometheus_optimized
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1"]

  # =========================
  # Edge Computing Node
  # =========================
  
  edge_node:
    image: nginx:alpine
    container_name: edge_node
    volumes:
      - ./config/nginx/edge-cache.conf:/etc/nginx/nginx.conf:ro
      - ./cache/edge:/var/cache/nginx
      - ./logs/nginx:/var/log/nginx
    networks:
      - edge_network
      - media_network
    ports:
      - "8888:80"
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost/health"]

  # =========================
  # Container Auto-Scaler
  # =========================
  
  autoscaler:
    image: nginx/nginx-prometheus-exporter:latest
    container_name: container_autoscaler
    build:
      context: ./autoscaler
      dockerfile: Dockerfile
    environment:
      - DOCKER_HOST=unix:///var/run/docker.sock
      - PROMETHEUS_URL=http://prometheus_optimized:9090
      - SCALING_RULES_PATH=/etc/autoscaler/rules.yml
      - CHECK_INTERVAL=30s
      - COOLDOWN_PERIOD=300s
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./config/autoscaler:/etc/autoscaler:ro
    networks:
      - media_network
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M

  # =========================
  # Performance Analytics Engine
  # =========================
  
  analytics_engine:
    image: python:3.11-slim
    container_name: analytics_engine
    build:
      context: ./analytics
      dockerfile: Dockerfile
    environment:
      - REDIS_URL=redis://neural_cache:6379
      - PROMETHEUS_URL=http://prometheus_optimized:9090
      - POSTGRES_URL=postgresql://${DB_USER:-postgres}:${DB_PASSWORD:-postgres}@postgres_primary:5432/${DB_NAME:-mediaserver}
      - ML_MODEL_PATH=/models/performance_predictor.joblib
    volumes:
      - ./ml-models:/models:ro
      - ./analytics:/app
      - ./logs/analytics:/var/log/analytics
    networks:
      - media_network
      - cache_network
    restart: unless-stopped
    logging: *default-logging
    depends_on:
      - neural_cache
      - postgres_primary
      - prometheus_optimized
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G

# =========================
# System Optimizations
# =========================

configs:
  kernel_tuning:
    external: true
  
  docker_daemon_config:
    external: true

secrets:
  api_keys:
    external: true
  
  certificates:
    external: true